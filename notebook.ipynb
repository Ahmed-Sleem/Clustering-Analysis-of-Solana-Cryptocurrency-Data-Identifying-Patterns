{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Xuj2kR4wn_gv",
        "y05GAkzAimOe",
        "Q7C_VBSuoIpP",
        "1rCRHTvdb-tt",
        "RCqCU3qEdKxd",
        "saqiLhF5daUB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#prerun"
      ],
      "metadata": {
        "id": "Xuj2kR4wn_gv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9dwLPtDn-G4"
      },
      "outputs": [],
      "source": [
        "!pip install tslearn\n",
        "!pip install mplfinance\n",
        "!pip install dtw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import mplfinance as mpf\n",
        "import math\n",
        "from io import StringIO\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "from joblib import load\n",
        "import csv\n",
        "import shutil\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import glob"
      ],
      "metadata": {
        "id": "QR6ZELOIoDLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##for colab only"
      ],
      "metadata": {
        "id": "y05GAkzAimOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "8mkoG9CLikYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#main"
      ],
      "metadata": {
        "id": "Q7C_VBSuoIpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#functions\n",
        "\n",
        "#preprocess data to points\n",
        "def pips(data: np.array, n_pips: int, dist_measure: int):\n",
        "\n",
        "    # dist_measure\n",
        "    # 1 = Euclidean Distance\n",
        "    # 2 = Perpindicular Distance\n",
        "    # 3 = Vertical Distance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #first phase converting\n",
        "    n_pips = n_pips + 2\n",
        "\n",
        "\n",
        "    pips_x = [0, len(data) - 1]  # Index\n",
        "    pips_y = [data[0], data[-1]] # Price\n",
        "\n",
        "    for curr_point in range(2, n_pips):\n",
        "\n",
        "        md = 0.0 # Max distance\n",
        "        md_i = -1 # Max distance index\n",
        "        insert_index = -1\n",
        "\n",
        "        for k in range(0, curr_point - 1):\n",
        "\n",
        "            # Left adjacent, right adjacent indices\n",
        "            left_adj = k\n",
        "            right_adj = k + 1\n",
        "\n",
        "            time_diff = pips_x[right_adj] - pips_x[left_adj]\n",
        "            price_diff = pips_y[right_adj] - pips_y[left_adj]\n",
        "            slope = price_diff / time_diff\n",
        "            intercept = pips_y[left_adj] - pips_x[left_adj] * slope;\n",
        "\n",
        "            for i in range(pips_x[left_adj] + 1, pips_x[right_adj]):\n",
        "\n",
        "                d = 0.0 # Distance\n",
        "                if dist_measure == 1: # Euclidean distance\n",
        "                    d =  ( (pips_x[left_adj] - i) ** 2 + (pips_y[left_adj] - data[i]) ** 2 ) ** 0.5\n",
        "                    d += ( (pips_x[right_adj] - i) ** 2 + (pips_y[right_adj] - data[i]) ** 2 ) ** 0.5\n",
        "                elif dist_measure == 2: # Perpindicular distance\n",
        "                    d = abs( (slope * i + intercept) - data[i] ) / (slope ** 2 + 1) ** 0.5\n",
        "                else: # Vertical distance\n",
        "                    d = abs( (slope * i + intercept) - data[i] )\n",
        "\n",
        "                if d > md:\n",
        "                    md = d\n",
        "                    md_i = i\n",
        "                    insert_index = right_adj\n",
        "\n",
        "        pips_x.insert(insert_index, md_i)\n",
        "        pips_y.insert(insert_index, data[md_i])\n",
        "\n",
        "\n",
        "\n",
        "     #second phase converting\n",
        "    points_no = len(pips_x)  #number of points\n",
        "    points = [] #list of new converting data\n",
        "\n",
        "\n",
        "    #the rate of change of y over the rate of change of x for every point\n",
        "    for i in range(2,points_no) :\n",
        "\n",
        "      x = ((pips_x[i] - pips_x[i-1])/ pips_x[i-1] ) * 100\n",
        "      y = ((pips_y[i] - pips_y[i-1])/ pips_y[i-1] ) * 100\n",
        "\n",
        "      z = y / x\n",
        "\n",
        "      points.append(z)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return points\n",
        "\n",
        "\n",
        "\n",
        "#creating points dataset\n",
        "def create_dataset(csv_file,no_of_points_in_raw,window_size=24,step=5):\n",
        "\n",
        "    #load data\n",
        "    data = pd.read_csv(csv_file)\n",
        "    data['date'] = data['date'].astype('datetime64[s]')\n",
        "    data = data.set_index('date')\n",
        "\n",
        "\n",
        "    #dataset\n",
        "    x = data['close'].to_numpy()\n",
        "\n",
        "\n",
        "\n",
        "    #create dataset\n",
        "    dataset = pd.DataFrame()\n",
        "\n",
        "\n",
        "    # Iterate through the data\n",
        "    for i in range(0, len(x) - window_size + 1, step):\n",
        "        window_data = x[i:i+window_size]\n",
        "\n",
        "\n",
        "        #get points\n",
        "        points = pips(window_data, no_of_points_in_raw, 2)\n",
        "\n",
        "\n",
        "        #creating the new record\n",
        "        new_record = {\n",
        "            'points': points\n",
        "\n",
        "        }\n",
        "\n",
        "        # Convert the new record to a DataFrame\n",
        "        new_df = pd.DataFrame([new_record])\n",
        "\n",
        "        # Concatenate the existing DataFrame with the new record DataFrame\n",
        "        dataset = pd.concat([dataset, new_df], ignore_index=True)\n",
        "\n",
        "    # Save the updated DataFrame to CSV\n",
        "    dataset.to_csv('dataset.csv', index=False)\n",
        "\n",
        "    #return\n",
        "    return dataset\n",
        "\n",
        "\n",
        "#clustering and saving clusters data\n",
        "def cluster_and_save(dataset, n_clusters):\n",
        "    # Preprocess data (scaling)\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(dataset)\n",
        "\n",
        "    # Apply K-means clustering\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(scaled_data)\n",
        "\n",
        "    # Create a folder to save clusters if it doesn't exist\n",
        "    if not os.path.exists('clusters'):\n",
        "        os.makedirs('clusters')\n",
        "\n",
        "    # Convert numpy array to DataFrame\n",
        "    dataset_df = pd.DataFrame(dataset, columns=[f'Feature_{i+1}' for i in range(dataset.shape[1])])\n",
        "\n",
        "    # Iterate over each cluster\n",
        "    for cluster_num in range(n_clusters):\n",
        "        # Filter data points belonging to the current cluster\n",
        "        cluster_data = dataset_df[cluster_labels == cluster_num]\n",
        "\n",
        "        # Convert each row to a list and save as a single row containing a list of features\n",
        "        cluster_data_list = cluster_data.values.tolist()\n",
        "\n",
        "        # Save the cluster data to a file\n",
        "        file_name = f'cluster_{cluster_num + 1}.csv'\n",
        "        file_path = os.path.join('clusters', file_name)\n",
        "        with open(file_path, 'w') as file:\n",
        "            for row in cluster_data_list:\n",
        "                file.write(','.join(map(str, row)) + '\\n')\n",
        "        print(f\"Cluster {cluster_num + 1} saved to: {file_path}\")\n",
        "\n",
        "\n",
        "#determine the best number of clusters\n",
        "def determine_optimal_clusters(dataset, cluster_range):\n",
        "    # Preprocess data (scaling)\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(dataset)\n",
        "\n",
        "    # Initialize variables to store silhouette scores\n",
        "    silhouette_scores = []\n",
        "\n",
        "    # Iterate over different number of clusters\n",
        "    for n_clusters in cluster_range:\n",
        "        # Apply K-means clustering\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        cluster_labels = kmeans.fit_predict(scaled_data)\n",
        "\n",
        "        # Calculate silhouette score\n",
        "        silhouette_avg = silhouette_score(scaled_data, cluster_labels)\n",
        "        silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "    # Find the number of clusters with the highest silhouette score\n",
        "    optimal_clusters = cluster_range[np.argmax(silhouette_scores)]\n",
        "\n",
        "    return optimal_clusters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#full clustering sequence\n",
        "def clustering(data_file,row_points,window,step,range_clusters=0,no_of_clusters=0):\n",
        "\n",
        "  #preprocessing\n",
        "  create_dataset(data_file,row_points,window,step)\n",
        "\n",
        "  print(\"preprocessing done !\")\n",
        "\n",
        "  # Load dataset from CSV\n",
        "  dataset = pd.read_csv('dataset.csv')\n",
        "\n",
        "  # Convert string representation of points to lists of floats\n",
        "  dataset['points'] = dataset['points'].apply(eval)\n",
        "\n",
        "  # Convert points to numpy array\n",
        "  points_array = np.array(dataset['points'].tolist())\n",
        "\n",
        "  print(\"load points done !\")\n",
        "\n",
        "\n",
        "  if no_of_clusters == 0 :\n",
        "\n",
        "    # Load dataset from CSV\n",
        "    dataset = pd.read_csv('dataset.csv')\n",
        "\n",
        "    # Convert string representation of points to lists of floats\n",
        "    dataset['points'] = dataset['points'].apply(eval)\n",
        "\n",
        "\n",
        "\n",
        "    # Convert points to numpy array\n",
        "    points_array = np.array(dataset['points'].tolist())\n",
        "\n",
        "\n",
        "\n",
        "    # Define range of cluster numbers to test\n",
        "    cluster_range = range_clusters\n",
        "\n",
        "    # Determine the optimal number of clusters using the Silhouette Method\n",
        "    optimal_clusters = determine_optimal_clusters(points_array, cluster_range)\n",
        "\n",
        "    print(\"Optimal number of clusters:\", optimal_clusters)\n",
        "\n",
        "\n",
        "\n",
        "    # Define the number of clusters\n",
        "    n_clusters = int(optimal_clusters)\n",
        "\n",
        "\n",
        "  elif no_of_clusters != 0 :\n",
        "    print(\"start clustering to the chosen clusters number\")\n",
        "    n_clusters = no_of_clusters\n",
        "\n",
        "  else:\n",
        "    print(\"error please choose a range or a number of cluster to start clustering !\")\n",
        "\n",
        "  print(\"start clustering\")\n",
        "\n",
        "  # Perform clustering and save clusters\n",
        "  cluster_and_save(points_array, n_clusters)\n",
        "\n",
        "  print(\"clustering done !\")\n",
        "\n",
        "\n",
        "#to plot clusters and visualize patterns\n",
        "def plot_clusters(cluster_folder='clusters',points=5):\n",
        "    # Get list of cluster files\n",
        "    cluster_files = [file for file in os.listdir(cluster_folder) if file.startswith('cluster_')]\n",
        "\n",
        "    # Plot each cluster separately\n",
        "    for file in cluster_files:\n",
        "        # Read cluster data from CSV\n",
        "        cluster_data = pd.read_csv(os.path.join(cluster_folder, file))\n",
        "\n",
        "        # Plot cluster data\n",
        "        plt.figure(figsize=(10, points+1))\n",
        "        for i in range(len(cluster_data)):\n",
        "            plt.plot(cluster_data.iloc[i].values, label=f'Record {i+1}')  # Plot each record separately\n",
        "\n",
        "        plt.xlabel('Feature Index')\n",
        "        plt.ylabel('Feature Value')\n",
        "        plt.title(f'Cluster {file[:-4]} Records Overlay Plot')\n",
        "\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "#to save plots\n",
        "def save_cluster_plots(cluster_folder='clusters', points=5, save_folder='cluster_plots'):\n",
        "    if not os.path.isdir(save_folder):\n",
        "        os.makedirs(save_folder)\n",
        "\n",
        "    # Get list of cluster files\n",
        "    cluster_files = [file for file in os.listdir(cluster_folder) if file.startswith('cluster_')]\n",
        "\n",
        "    # Plot each cluster separately\n",
        "    for file in cluster_files:\n",
        "        # Read cluster data from CSV\n",
        "        cluster_data = pd.read_csv(os.path.join(cluster_folder, file))\n",
        "\n",
        "        # Plot cluster data\n",
        "        plt.figure(figsize=(10, points+1))\n",
        "        for i in range(len(cluster_data)):\n",
        "            plt.plot(cluster_data.iloc[i].values, label=f'Record {i+1}')  # Plot each record separately\n",
        "\n",
        "        plt.xlabel('Feature Index')\n",
        "        plt.ylabel('Feature Value')\n",
        "        plt.title(f'Cluster {file[:-4]} Records Overlay Plot')\n",
        "\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.savefig(os.path.join(save_folder, f'cluster_{file[:-4]}.png'))  # Save plot as PNG file\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_clusters(folder_path, no_of_clusters):\n",
        "    \"\"\"Load cluster data from CSV files in the specified folder.\"\"\"\n",
        "    cluster_data = []\n",
        "    # Get a list of files in the folder\n",
        "    files = os.listdir(folder_path)\n",
        "    # Iterate over the files\n",
        "    for file_name in files:\n",
        "        # Check if the file is a CSV file and matches the cluster naming convention\n",
        "        if file_name.endswith('.csv') and file_name.startswith('cluster_'):\n",
        "            # Extract the cluster number from the file name\n",
        "            cluster_number = int(file_name.split('_')[1].split('.')[0])\n",
        "            # Check if the cluster number is within the specified range\n",
        "            if cluster_number <= no_of_clusters:\n",
        "                file_path = os.path.join(folder_path, file_name)\n",
        "                cluster_df = pd.read_csv(file_path)\n",
        "                cluster_data.append(cluster_df)\n",
        "    return cluster_data\n",
        "\n",
        "#cluster detection\n",
        "def detect_cluster(new_record, cluster_data, k):\n",
        "    \"\"\"Detect the cluster that a new record belongs to using KNN.\"\"\"\n",
        "    distances = []\n",
        "    for cluster_df in cluster_data:\n",
        "        num_samples = cluster_df.shape[0]  # Get the number of samples in the cluster dataset\n",
        "        if k > num_samples:\n",
        "            k = num_samples  # Adjust k if it's greater than the number of samples\n",
        "        X = cluster_df.values  # Convert cluster data to numpy array\n",
        "        nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(X)\n",
        "        distances.append(nbrs.kneighbors([new_record])[0][0].mean())\n",
        "    # Assign the new record to the cluster with the smallest average distance\n",
        "    cluster_index = np.argmin(distances)\n",
        "    return cluster_index + 1  # Return cluster index (starting from 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#count clusters\n",
        "def count_cluster(folder_path=\"clusters\"):\n",
        "    if not os.path.isdir(folder_path):\n",
        "        return \"Invalid folder path\"\n",
        "\n",
        "    file_count = 0\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.startswith(\"cluster_\") and file.endswith(\".csv\"):\n",
        "            file_count += 1\n",
        "\n",
        "\n",
        "    print(f'number of cluster = {file_count}')\n",
        "    return file_count\n",
        "\n",
        "#delete the empty clusters\n",
        "def delete_empty(folder_path=\"clusters\"):\n",
        "    if not os.path.isdir(folder_path):\n",
        "        return \"Invalid folder path\"\n",
        "\n",
        "    files_to_rename = []\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith(\".csv\"):\n",
        "            file_path = os.path.join(folder_path, file)\n",
        "            with open(file_path, 'r') as csv_file:\n",
        "                csv_reader = csv.reader(csv_file)\n",
        "                num_records = sum(1 for _ in csv_reader)  # Count the number of records\n",
        "                if num_records < 2:\n",
        "                    os.remove(file_path)\n",
        "                    print(f\"Deleted {file} due to less than 2 records.\")\n",
        "                else:\n",
        "                    files_to_rename.append(file)\n",
        "\n",
        "\n",
        "\n",
        "def organize(folder_path=\"clusters\"):\n",
        "\n",
        "  data = os.path.abspath(folder_path)\n",
        "  for i, f in enumerate(os.listdir(data)):\n",
        "      src = os.path.join(data, f)\n",
        "      dst = os.path.join(data, (f\"cluster_{str(i + 1)}.csv\"))\n",
        "      os.rename(src, dst)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#full detection sequence\n",
        "def detect(records,no_of_clusters,no_of_points_in_raw,clusters_folder=\"clusters\" ):\n",
        "\n",
        "\n",
        "  #loading clusters\n",
        "  cluster_data = load_clusters(clusters_folder,no_of_clusters)\n",
        "\n",
        "\n",
        "  #dataset\n",
        "  x = np.array(records)\n",
        "\n",
        "\n",
        "  #get points\n",
        "  points = pips(x, no_of_points_in_raw, 2)\n",
        "\n",
        "\n",
        "  #predict cluster\n",
        "  new_record = np.array(points)\n",
        "  predicted_cluster = detect_cluster(new_record, cluster_data,no_of_clusters)\n",
        "\n",
        "\n",
        "  #printing\n",
        "  print(f\"cluster : {predicted_cluster}\")\n",
        "\n",
        "  return predicted_cluster\n",
        "\n",
        "\n",
        "\n",
        "def clear_clusters(folder_path=\"clusters\"):\n",
        "    if not os.path.isdir(folder_path):\n",
        "        print(\"Folder does not exist.\")\n",
        "        return\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        try:\n",
        "            if os.path.isfile(file_path):\n",
        "                os.unlink(file_path)  # Delete file\n",
        "            elif os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)  # Delete directory and its contents\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def download_plots(folder=\"cluster_plots\"):\n",
        "  files.download(f\"{folder}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bPb0q3WQoPYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#variables"
      ],
      "metadata": {
        "id": "1ku_vR1Cbt3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first clustering\n",
        "data = \"sol.csv\" #data file (solana)\n",
        "points = 5 #number of points in each record\n",
        "window = 24 #window size\n",
        "step = 10 #step size between every window and the next\n",
        "clusters = range(25,50) #range of clusters number"
      ],
      "metadata": {
        "id": "G5f7Zd3ybx7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#clustering"
      ],
      "metadata": {
        "id": "1rCRHTvdb-tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#clear old clusters\n",
        "clear_clusters()\n",
        "\n",
        "#clustering\n",
        "clustering(data ,points ,window ,step ,clusters )\n",
        "\n",
        "#cleaning empty clusters\n",
        "delete_empty()\n",
        "\n",
        "\n",
        "#get the number of clusters after cleaning\n",
        "clusters = count_cluster()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AWOFW6aJ6-i",
        "outputId": "7b98d2ac-dbe1-42de-dfb9-e947a18d36f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocessing done !\n",
            "load points done !\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of clusters: 43\n",
            "start clustering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 1 saved to: clusters/cluster_1.csv\n",
            "Cluster 2 saved to: clusters/cluster_2.csv\n",
            "Cluster 3 saved to: clusters/cluster_3.csv\n",
            "Cluster 4 saved to: clusters/cluster_4.csv\n",
            "Cluster 5 saved to: clusters/cluster_5.csv\n",
            "Cluster 6 saved to: clusters/cluster_6.csv\n",
            "Cluster 7 saved to: clusters/cluster_7.csv\n",
            "Cluster 8 saved to: clusters/cluster_8.csv\n",
            "Cluster 9 saved to: clusters/cluster_9.csv\n",
            "Cluster 10 saved to: clusters/cluster_10.csv\n",
            "Cluster 11 saved to: clusters/cluster_11.csv\n",
            "Cluster 12 saved to: clusters/cluster_12.csv\n",
            "Cluster 13 saved to: clusters/cluster_13.csv\n",
            "Cluster 14 saved to: clusters/cluster_14.csv\n",
            "Cluster 15 saved to: clusters/cluster_15.csv\n",
            "Cluster 16 saved to: clusters/cluster_16.csv\n",
            "Cluster 17 saved to: clusters/cluster_17.csv\n",
            "Cluster 18 saved to: clusters/cluster_18.csv\n",
            "Cluster 19 saved to: clusters/cluster_19.csv\n",
            "Cluster 20 saved to: clusters/cluster_20.csv\n",
            "Cluster 21 saved to: clusters/cluster_21.csv\n",
            "Cluster 22 saved to: clusters/cluster_22.csv\n",
            "Cluster 23 saved to: clusters/cluster_23.csv\n",
            "Cluster 24 saved to: clusters/cluster_24.csv\n",
            "Cluster 25 saved to: clusters/cluster_25.csv\n",
            "Cluster 26 saved to: clusters/cluster_26.csv\n",
            "Cluster 27 saved to: clusters/cluster_27.csv\n",
            "Cluster 28 saved to: clusters/cluster_28.csv\n",
            "Cluster 29 saved to: clusters/cluster_29.csv\n",
            "Cluster 30 saved to: clusters/cluster_30.csv\n",
            "Cluster 31 saved to: clusters/cluster_31.csv\n",
            "Cluster 32 saved to: clusters/cluster_32.csv\n",
            "Cluster 33 saved to: clusters/cluster_33.csv\n",
            "Cluster 34 saved to: clusters/cluster_34.csv\n",
            "Cluster 35 saved to: clusters/cluster_35.csv\n",
            "Cluster 36 saved to: clusters/cluster_36.csv\n",
            "Cluster 37 saved to: clusters/cluster_37.csv\n",
            "Cluster 38 saved to: clusters/cluster_38.csv\n",
            "Cluster 39 saved to: clusters/cluster_39.csv\n",
            "Cluster 40 saved to: clusters/cluster_40.csv\n",
            "Cluster 41 saved to: clusters/cluster_41.csv\n",
            "Cluster 42 saved to: clusters/cluster_42.csv\n",
            "Cluster 43 saved to: clusters/cluster_43.csv\n",
            "clustering done !\n",
            "Deleted cluster_5.csv due to less than 2 records.\n",
            "Deleted cluster_17.csv due to less than 2 records.\n",
            "Deleted cluster_14.csv due to less than 2 records.\n",
            "Deleted cluster_35.csv due to less than 2 records.\n",
            "Deleted cluster_22.csv due to less than 2 records.\n",
            "Deleted cluster_19.csv due to less than 2 records.\n",
            "Deleted cluster_28.csv due to less than 2 records.\n",
            "number of cluster = 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ploting clusters"
      ],
      "metadata": {
        "id": "RCqCU3qEdKxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting if needed\n",
        "plot_clusters()\n"
      ],
      "metadata": {
        "id": "fNNQhTfMbQkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clear old images\n",
        "clear_clusters(\"cluster_plots\")\n",
        "\n",
        "\n",
        "#save plots\n",
        "save_cluster_plots()"
      ],
      "metadata": {
        "id": "f-6Al-09hoAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#cluster detection"
      ],
      "metadata": {
        "id": "saqiLhF5daUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get the number of clusters after cleaning\n",
        "clusters = count_cluster()\n",
        "\n",
        "\n",
        "#pattern detection 24 record\n",
        "close_prices = [\n",
        "    19.845, 19.861, 19.886, 19.823, 19.826, 19.806, 19.765, 19.78, 19.763, 19.765,\n",
        "    19.826, 19.832, 19.881, 19.848, 19.77, 19.805, 19.779, 19.786, 19.663, 19.672,\n",
        "    19.576, 19.539, 19.499, 19.374\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "the_detected_clustesr = detect(close_prices,clusters,points)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trECL6Xwbfga",
        "outputId": "09f3705d-34f9-4c04-af67-3284ab5ba64d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cluster : 25\n"
          ]
        }
      ]
    }
  ]
}